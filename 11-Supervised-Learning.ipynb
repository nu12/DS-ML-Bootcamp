{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Algorithms are trained using *labeled* examples, such as an input where the **desired output is known**. Example: *Spam* vs *Legitimate* Email / *Positive* vs *Negative* Movei Review.\n",
    "\n",
    "Supervied Learning is commonly used in applications where historical data predicts likely future events.\n",
    "\n",
    "Steps:\n",
    "* Data Acquisition.\n",
    "* Data Cleaning.\n",
    "* Split Data.\n",
    "    * Training Data: used to train model parameters.\n",
    "    * Validation Data: used to determine what model hyperparameters to adjust.\n",
    "    * Test Data: used to get some final performance metrics (expected real-world performance).\n",
    "* Model Fitting using training data.\n",
    "* Model Testing.\n",
    "    * Adjust Model Paramenters using validation data.\n",
    "    * Iterate between model training and model testing.\n",
    "* Model Deployment.\n",
    "\n",
    "## Evaluating performance\n",
    "\n",
    "\"Good\" and \"bad\" values for each performance metric depend on the context and specific circumstances. There is no magick number to evaluate whether a given performance metric is good or not.\n",
    "\n",
    "### Classification Error Metrics\n",
    "Key classification metrics:\n",
    "* Accuracy: number of correct predictions made by the model divided by the total number of predictions.\n",
    "    * Higher is better.\n",
    "    * Useful when target classes (Y) are **well balanced**.\n",
    "* Recall: number of true positives divided by the number of true positives plus the number of false negatives.\n",
    "    * Ability to find **all** the relevant cases within a dataset.\n",
    "    * Useful when target classes (Y) are **unbalanced**.\n",
    "    * Minimizes false negatives.\n",
    "* Precision: number of true positives divided by the number of true positives plus the number of false positives.\n",
    "    * Ability to identify **only** the data points that were actually relevant.\n",
    "    * Useful when target classes (Y) are **unbalanced**.\n",
    "    * Minimizes false positives.\n",
    "* F1-Score: $ F_{1} = 2 * \\frac{precision * recall}{precision + recall}$\n",
    "    * Optimal blend of precision and recall.\n",
    "    * Harmonic mean punishes extreme values giving a fair assessment of the combination.\n",
    "* Confusion matrix: Real values vs Predicted values.\n",
    "\n",
    "\n",
    "### Regression Errror Metrics\n",
    "Regression is a task when a model attempts to predict continuous values.\n",
    "* Mean Absolute Error (MAE): mean of the absolute value of errors. It's simple, however it won't punish large errors.\n",
    "* Mean Squared Error (MSE): mean of the squared error. Greater punishment for outliers. Issue: it also squares the predicted units, making it more difficult to interpret.\n",
    "* Root Mean Square Error (RMSE): take the square root of the MSE.\n",
    "    * Most common error metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Scikit-Learn\n",
    "\n",
    "All model are available through estimators (model classes).\n",
    "\n",
    "General form:\n",
    "```python\n",
    "from sklean.family import model\n",
    "```\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from sklean.linear_model import LinearRegression\n",
    "```\n",
    "\n",
    "## Instanciating\n",
    "Estimators have suitable default values.\n",
    "\n",
    "General form:\n",
    "```python\n",
    "model = ModelName(parameter='value')\n",
    "```\n",
    "\n",
    "Example:\n",
    "```python\n",
    "model = LinearRegression(normalize=True)\n",
    "print(model)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Split training & test data\n",
    "\n",
    "General form:\n",
    "```python\n",
    "from sklean.cross_validation import train_test_split\n",
    "```\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "Y: [0, 1, 2, 3, 4]\n",
      "X train: [[2 3]\n",
      " [0 1]\n",
      " [8 9]]\n",
      "Y train: [1, 0, 4]\n",
      "X test: [[4 5]\n",
      " [6 7]]\n",
      "Y test: [2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = np.arange(10).reshape((5,2)), range(5)\n",
    "print(\"X:\",x)\n",
    "print(\"Y:\",list(y))\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n",
    "print(\"X train:\",x_train)\n",
    "print(\"Y train:\",y_train)\n",
    "print(\"X test:\",x_test)\n",
    "print(\"Y test:\",y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model on data\n",
    "\n",
    "General form:\n",
    "```python\n",
    "model.fit(x_train, y_train)\n",
    "```\n",
    "\n",
    "## Predict values on the test data\n",
    "\n",
    "General form:\n",
    "```python\n",
    "predictions = model.predict(x_test)\n",
    "```\n",
    "\n",
    "### Other prediction methods\n",
    "\n",
    "Predict probability of each category:\n",
    "```python\n",
    "model.predict_proba()\n",
    "```\n",
    "\n",
    "Calculate score values:\n",
    "```python\n",
    "model.score()\n",
    "```\n",
    "\n",
    "## Evaluate the model\n",
    "\n",
    "Compare predictions on test data againt y_test values.\n",
    "\n",
    "The method depends on which ML algorithm is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Trade-Off\n",
    "\n",
    "The bias-variance trade-off is the point where we are adding noise by adding model complexity (flexibility) without increasing the performance of the model on unseen data. The training error goes down as it has to, but the test error starts to go up. This is also known as overfitting. \n",
    "\n",
    "![Bias Variance Trade-Off](https://i0.wp.com/www.coriers.com/wp-content/uploads/2019/06/bias-graph-analytics.png?resize=894%2C599&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
